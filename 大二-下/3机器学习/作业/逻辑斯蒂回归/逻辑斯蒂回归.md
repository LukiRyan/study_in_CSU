

逻辑斯蒂回归是一种常见的二分类算法，它也可以用于解决连续变量的回归问题，通过从训练数据中学习得到一个能够将样本分为不同类别的分类函数。其理论基础建立在条件概率和极大似然估计基础之上。

## 回归分析理论依据

假设训练数据集 $\{(x_1, y_1), (x_2, y_2),\dots,(x_n, y_n)\}$，其中 $x_i \in \mathcal{X} \subseteq R^n$，$y_i \in \{0, 1\}$ 表示对应的标签，逻辑斯蒂回归模型可以表示为：

$$\hat{y} = P(y=1|x;w) = \frac{1}{1 + e^{-w^Tx}}$$

其中 $w \in \mathbb{R}^n$ 是需要学习的权重参数，$\hat{y}$ 表示给定输入 $x$，预测其属于类别 $1$ 的可能性。

使用极大似然估计方法，我们需要最大化似然函数：

$$L(w) = \prod_{i=1}^n P(y_i|x_i ; w)^{y_i} (1 - P(y_i|x_i ; w))^{1-y_i}$$

为了方便计算，通常采用对数似然函数形式，即：

$$l(w) = \ln L(w) = \sum_{i=1}^n [y_i \log P(y_i|x_i ; w) + (1-y_i) \ln(1 - P(y_i|x_i ; w))]$$

对 $l(w)$ 求偏导可以得到损失函数：

$$\begin{aligned}
J(w) &= \sum_{i=1}^n [y_i \ln P(y_i|x_i ; w) + (1-y_i) \ln(1 - P(y_i|x_i ;w))] \\
&=-\left[\frac{1}{n}\sum_{i=1}^{n} y_i \ln \hat{y}_i+(1-y_i) \ln (1-\hat{y}_i)\right]
\end{aligned}$$

其中 $\hat{y}_i$ 是预测的类别概率，$y_i$ 是样本实际标签。我们发现是将逻辑斯蒂模型中 $P(y=1|x;w)$ 的定义代入后的结果。由此可知，逻辑斯蒂回归学习的是使得损失函数最小化的参数 $w$，即最大化似然函数，从而拟合训练数据集。

## 分类理论依据

给定一个输入特征向量 $x$，逻辑斯蒂回归预测输出变量 $y$ 取值 $\{0,1\}$ 中的一个，并通过学习权重向量 $w$ 来实现这一过程。

具体而言，逻辑斯蒂回归的基本思想是将输入特征向量 $x$ 通过一个线性组合与权重向量 $w$ 相乘得到一个分数，然后通过 Sigmoid 函数对这个分数进行转换，并预测一个或零概率。

该模型的数学公式表示为：

$$y=\sigma(w^{T}x)=\frac{1}{1+e^{-w^{T}x}}$$

其中 $\sigma(z)=\frac{1}{1+e^{-z}}$ 表示 sigmoid 函数。$w^Tx$ 在这里是控制输出目标为 $1$ 的分数，根据 sigmoid 函数的性质，sigmoid 函数输出越接近 $1$，那么目标变量取值为 $1$ 的可能性就越大，反之也成立。

逻辑斯蒂回归使用交叉熵作为损失函数来训练模型。如果我们设真实结果为 $t$，则交叉熵误差可以定义为：

$$J(w)=-\frac{1}{n}\sum_{i=1}^{n}[t_i\log(y_i)+(1-t_i)\log(1-y_i)]$$

其中 $n$ 是训练样本数，$t_i$ 表示真实的目标变量取值，$y_i$ 是模型预测目标变量取值的概率（即 sigmoid 函数输出）。

然后通过使用随机梯度下降等优化算法来最小化损失函数 $J(w)$，得到学习好的权重向量 $w$。在测试阶段，我们可以将输入特征向量 $x$ 按照 $y=\sigma(w^{T}x)$ 的方式进行预测，如果 $y$ 大于等于 $0.5$ 则预测为类别 1，否则预测为类别 0。

因此，逻辑斯蒂回归利用 Sigmoid 函数对线性加权组合来描述输入特征和输出变量之间的关系，通过最小化交叉熵损失函数进行模型训练和分类预测，以达到二分类问题的分类。

参考教材P59页证明